{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction to DataFrames - Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id='123456', name='Computer Science')\n",
      "Row(firstName='xiangrui', lastName='meng', email='no-reply@stanford.edu', salary=120000)\n",
      "no-reply@berkeley.edu\n"
     ]
    }
   ],
   "source": [
    "# import pyspark class Row from module Sql\n",
    "from pyspark.sql import *\n",
    "\n",
    "# Create Example Data - Departments and Employees\n",
    "\n",
    "# Create the Departments\n",
    "department1 = Row(id='123456', name='Computer Science')\n",
    "department2 = Row(id='789012', name='Mechanical Engineering')\n",
    "department3 = Row(id='345678', name='Theater and Drama')\n",
    "department4 = Row(id='901234', name='Indoor Recreation')\n",
    "\n",
    "# Create the Employees\n",
    "Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\n",
    "employee1 = Employee('michael', 'armbrust', 'no-reply@berkeley.edu', 100000)\n",
    "employee2 = Employee('xiangrui', 'meng', 'no-reply@stanford.edu', 120000)\n",
    "employee3 = Employee('matei', None, 'no-reply@waterloo.edu', 140000)\n",
    "employee4 = Employee(None, 'wendell', 'no-reply@berkeley.edu', 160000)\n",
    "\n",
    "# Create the DepartmentWithEmployees instances from Departments and Employees\n",
    "departmentWithEmployees1 = Row(department=department1, employees=[employee1, employee2])\n",
    "departmentWithEmployees2 = Row(department=department2, employees=[employee3, employee4])\n",
    "departmentWithEmployees3 = Row(department=department3, employees=[employee1, employee4])\n",
    "departmentWithEmployees4 = Row(department=department4, employees=[employee2, employee3])\n",
    "\n",
    "print(department1)\n",
    "print(employee2)\n",
    "print(departmentWithEmployees1.employees[0].email)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create DataFrames from a list of the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[department: struct<id:string,name:string>, employees: array<struct<firstName:string,lastName:string,email:string,salary:bigint>>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[department: struct<id:string,name:string>, employees: array<struct<firstName:string,lastName:string,email:string,salary:bigint>>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"DataframesIntroduction\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "departmentsWithEmployeesSeq1 = [departmentWithEmployees1, departmentWithEmployees2]\n",
    "df1 = spark.createDataFrame(departmentsWithEmployeesSeq1)\n",
    "\n",
    "display(df1)\n",
    "\n",
    "departmentsWithEmployeesSeq2 = [departmentWithEmployees3, departmentWithEmployees4]\n",
    "df2 = spark.createDataFrame(departmentsWithEmployeesSeq2)\n",
    "\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          department|           employees|\n",
      "+--------------------+--------------------+\n",
      "|[123456, Computer...|[[michael, armbru...|\n",
      "|[789012, Mechanic...|[[matei,, no-repl...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|          department|           employees|\n",
      "+--------------------+--------------------+\n",
      "|[345678, Theater ...|[[michael, armbru...|\n",
      "|[901234, Indoor R...|[[xiangrui, meng,...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work with DataFrames. Union two DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          department|           employees|\n",
      "+--------------------+--------------------+\n",
      "|[123456, Computer...|[[michael, armbru...|\n",
      "|[789012, Mechanic...|[[matei,, no-repl...|\n",
      "|[345678, Theater ...|[[michael, armbru...|\n",
      "|[901234, Indoor R...|[[xiangrui, meng,...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unionDF = df1.unionAll(df2)\n",
    "unionDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the unioned DataFrame to a Parquet File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the file if it exists\n",
    "import os\n",
    "if os.path.exists(\"output/databricks-df-example.parquet\"):\n",
    "    os.remove(\"output/databricks-df-example.parquet\")\n",
    "unionDF.write.parquet(\"output/databricks-df-example.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read a DataFrame from the Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          department|           employees|\n",
      "+--------------------+--------------------+\n",
      "|[345678, Theater ...|[[michael, armbru...|\n",
      "|[789012, Mechanic...|[[matei,, no-repl...|\n",
      "|[901234, Indoor R...|[[xiangrui, meng,...|\n",
      "|[123456, Computer...|[[michael, armbru...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquetDF = spark.read.parquet(\"output/databricks-df-example.parquet\")\n",
    "parquetDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explode the employees column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[e: struct<firstName:string,lastName:string,email:string,salary:bigint>]\n",
      "+--------------------+\n",
      "|                   e|\n",
      "+--------------------+\n",
      "|[michael, armbrus...|\n",
      "|[xiangrui, meng, ...|\n",
      "|[matei,, no-reply...|\n",
      "|[, wendell, no-re...|\n",
      "|[michael, armbrus...|\n",
      "|[, wendell, no-re...|\n",
      "|[xiangrui, meng, ...|\n",
      "|[matei,, no-reply...|\n",
      "+--------------------+\n",
      "\n",
      "DataFrame[firstName: string, lastName: string, email: string, salary: bigint]\n",
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|               email|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "|     null| wendell|no-reply@berkeley...|160000|\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "|     null| wendell|no-reply@berkeley...|160000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "df = unionDF.select(explode(\"employees\").alias(\"e\"))\n",
    "print(df)\n",
    "df.show()\n",
    "explodeDF = df.selectExpr(\"e.firstName\", \"e.lastName\", \"e.email\", \"e.salary\")\n",
    "\n",
    "print(explodeDF)\n",
    "explodeDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|key|value|\n",
      "+---+-----+\n",
      "|  a|    b|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
    "eDF.select(explode(eDF.intlist).alias(\"anInt\")).collect()\n",
    "eDF.select(explode(eDF.mapfield).alias(\"key\", \"value\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use filter() to return the rows that match a predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[firstName: string, lastName: string, email: string, salary: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|               email|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filterDF = explodeDF.filter(explodeDF.firstName == \"xiangrui\").sort(explodeDF.lastName)\n",
    "display(filterDF)\n",
    "filterDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|               email|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, asc\n",
    "\n",
    "# Use `|`instead of `or`\n",
    "filterDF = explodeDF.filter((col(\"firstName\") == \"xiangrui\") | (col(\"firstName\") == \"michael\")).sort(asc(\"lastName\"))\n",
    "filterDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The where() clause is equivalent to filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|               email|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filterDF = explodeDF.where((col(\"firstName\") == \"xiangrui\") | (col(\"firstName\") == \"michael\")).sort(asc(\"lastName\"))\n",
    "filterDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace null values with -- using DataFrame Na function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|               email|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "|    matei|      --|no-reply@waterloo...|140000|\n",
      "|       --| wendell|no-reply@berkeley...|160000|\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "|       --| wendell|no-reply@berkeley...|160000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "|    matei|      --|no-reply@waterloo...|140000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nonNullDF = explodeDF.fillna(\"--\")\n",
    "nonNullDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve only rows with missing firstName or lastName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|               email|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "|     null| wendell|no-reply@berkeley...|160000|\n",
      "|     null| wendell|no-reply@berkeley...|160000|\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filterNonNullDF = explodeDF.filter(col(\"firstName\").isNull() | col(\"lastName\").isNull()).sort(\"email\")\n",
    "filterNonNullDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example aggregations using agg() and countDistinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[firstName: string, lastName: string, count(firstName): bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------------+\n",
      "|firstName|lastName|count(firstName)|\n",
      "+---------+--------+----------------+\n",
      "|     null| wendell|               0|\n",
      "| xiangrui|    meng|               2|\n",
      "|  michael|armbrust|               2|\n",
      "|    matei|    null|               2|\n",
      "+---------+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct, count\n",
    "\n",
    "countDistinctDF = explodeDF.select(\"firstName\", \"lastName\")\\\n",
    "    .groupBy(\"firstName\", \"lastName\")\\\n",
    "    .agg(count(\"firstName\"))\n",
    "\n",
    "display(countDistinctDF)\n",
    "countDistinctDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the DataFrame and SQL query physical plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) HashAggregate(keys=[firstName#179, lastName#180], functions=[count(firstName#179)])\n",
      "+- Exchange hashpartitioning(firstName#179, lastName#180, 200)\n",
      "   +- *(3) HashAggregate(keys=[firstName#179, lastName#180], functions=[partial_count(firstName#179)])\n",
      "      +- *(3) Project [e#173.firstName AS firstName#179, e#173.lastName AS lastName#180]\n",
      "         +- Generate explode(employees#1), false, [e#173]\n",
      "            +- Union\n",
      "               :- *(1) Project [employees#1]\n",
      "               :  +- Scan ExistingRDD[department#0,employees#1]\n",
      "               +- *(2) Project [employees#5]\n",
      "                  +- Scan ExistingRDD[department#4,employees#5]\n"
     ]
    }
   ],
   "source": [
    "countDistinctDF.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) HashAggregate(keys=[firstName#179, lastName#180], functions=[count(distinct firstName#179)])\n",
      "+- *(4) HashAggregate(keys=[firstName#179, lastName#180], functions=[partial_count(distinct firstName#179)])\n",
      "   +- *(4) HashAggregate(keys=[firstName#179, lastName#180, firstName#179], functions=[])\n",
      "      +- Exchange hashpartitioning(firstName#179, lastName#180, firstName#179, 200)\n",
      "         +- *(3) HashAggregate(keys=[firstName#179, lastName#180, firstName#179], functions=[])\n",
      "            +- *(3) Project [e#173.firstName AS firstName#179, e#173.lastName AS lastName#180]\n",
      "               +- Generate explode(employees#1), false, [e#173]\n",
      "                  +- Union\n",
      "                     :- *(1) Project [employees#1]\n",
      "                     :  +- Scan ExistingRDD[department#0,employees#1]\n",
      "                     +- *(2) Project [employees#5]\n",
      "                        +- Scan ExistingRDD[department#4,employees#5]\n"
     ]
    }
   ],
   "source": [
    "# register the DataFrame as a temp table so that we can query it using SQL\n",
    "explodeDF.registerTempTable(\"databricks_df_example\")\n",
    "\n",
    "# Perform the same query as the DataFrame above and return ``explain``\n",
    "countDistinctDF_sql = spark.sql(\"SELECT firstName, lastName, count(distinct firstName) \\\n",
    "                                as distinct_first_names FROM databricks_df_example GROUP BY firstName, lastName\")\n",
    "\n",
    "countDistinctDF_sql.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
